{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc0672d8-edf6-4d05-9a96-ae257e003b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "RAW_DATA_DIR = \"raw_data\"\n",
    "ANNOUNCEMENTS_DIR = os.path.join(RAW_DATA_DIR, \"announcements\")\n",
    "INTERMEETING_DIR = os.path.join(RAW_DATA_DIR, \"intermeeting\")\n",
    "PROCESSED_DIR = \"processed_data\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ea81272-68c7-4050-9662-a061b9f50c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(file_path):\n",
    "    \"\"\"Extract text from HTML files.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        text = soup.get_text(separator=' ')\n",
    "        \n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def find_fed_statement_section(text):\n",
    "    \"\"\"Find the actual FED statement in the HTML page.\"\"\"\n",
    "    # common indicators of the start of the actual statement\n",
    "    statement_indicators = [\n",
    "        \"For immediate release\",\n",
    "        \"Information received since\",\n",
    "        \"Recent indicators suggest\",\n",
    "        \"The Federal Open Market Committee\",\n",
    "        \"The Committee seeks\"\n",
    "    ]\n",
    "    \n",
    "    # earliest occurrence of any indicator\n",
    "    start_indices = [text.find(indicator) for indicator in statement_indicators if indicator in text]\n",
    "    if not start_indices:\n",
    "        return text\n",
    "    \n",
    "    start_idx = min(idx for idx in start_indices if idx >= 0)\n",
    "    \n",
    "    # finding end of the statement\n",
    "    end_indicators = [\n",
    "        \"Voting for the\",\n",
    "        \"Implementation Note\",\n",
    "        \"For media inquiries\",\n",
    "        \"Last Update:\"\n",
    "    ]\n",
    "    \n",
    "    end_indices = [text.find(indicator) for indicator in end_indicators if indicator in text and text.find(indicator) > start_idx]\n",
    "    if not end_indices:\n",
    "        return text[start_idx:]\n",
    "    \n",
    "    end_idx = min(end_indices)\n",
    "    return text[start_idx:end_idx]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()-]', '', text)\n",
    "    \n",
    "    text = re.sub(r'[.,;:!?](\\s*[.,;:!?])+', '. ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def simple_sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    A simple regex-based sentence tokenizer.\n",
    "    Splits text on common sentence terminators followed by whitespace and a capital letter.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    \n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) > 50: \n",
    "            sub_sentences = re.split(r'(?<=\\.)\\s+', sentence)\n",
    "            result.extend(sub_sentences)\n",
    "        else:\n",
    "            result.append(sentence)\n",
    "    \n",
    "    return [s.strip() for s in result if s.strip()]\n",
    "\n",
    "def identify_topic(sentence, topic_keywords):\n",
    "    \"\"\"\n",
    "    Identify if a sentence belongs to a specific topic.\n",
    "    Returns True if any primary keyword and context word pair is found within the window.\n",
    "    \"\"\"\n",
    "    primary_keywords = topic_keywords['primary_keywords']\n",
    "    context_words = topic_keywords['context_words']\n",
    "    window = topic_keywords['window']\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    for primary in primary_keywords:\n",
    "        if primary.lower() in sentence:\n",
    "            words = sentence.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if primary.lower() in word:\n",
    "                    start = max(0, i - window)\n",
    "                    end = min(len(words), i + window + 1)\n",
    "                    window_text = ' '.join(words[start:end])\n",
    "                    \n",
    "                    for context in context_words:\n",
    "                        if context.lower() in window_text:\n",
    "                            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40935964-8e9c-4bd0-9af7-da0a5102393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(directory, is_statement=True):\n",
    "    \"\"\"Process all HTML files in the given directory.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    monetary_policy_keywords = {\n",
    "        'primary_keywords': [\n",
    "            \"federal funds rate\", \"policy rate\", \"interest rate\", \"monetary policy stance\",\n",
    "            \"tightening\", \"easing\", \"accommodative policy\", \"restrictive policy\", \"basis points\", \n",
    "            \"target rate\", \"rate decision\", \"interest rate target\", \"tightening cycle\",\n",
    "            \"policy tightening\", \"rate easing\", \"policy easing\", \"rate cut\", \"rate hike\", \"bps\",\n",
    "            \"neutral stance\", \"normalization\", \"monetary tightening\", \"monetary easing\",\n",
    "            \"policy shift\", \"rate setting\", \"policy adjustment\", \"real interest rate\",\n",
    "            \"nominal interest rate\", \"terminal rate\", \"peak rate\", \"lower bound\", \"upper bound\"],\n",
    "        'context_words': [\n",
    "            \"increase\", \"decrease\", \"maintain\", \"adjust\", \"change\", \"vote\", \"decision\", \n",
    "            \"committee\", \"unanimous\", \"raise\", \"lower\", \"pause\", \"resume\", \"keep\", \"hold\", \"cut\",\n",
    "            \"hike\", \"boost\", \"reduce\", \"accelerate\", \"slow\", \"voted\", \"majority\", \"dissent\"],'window': 20}\n",
    "    \n",
    "    economic_conditions_keywords = {\n",
    "        'primary_keywords': [\n",
    "            \"inflation\", \"employment\", \"unemployment\", \"economic activity\", \"growth\",\n",
    "            \"GDP\", \"consumer spending\", \"business investment\", \"labor market\", \n",
    "            \"price stability\", \"economic outlook\", \"output\", \"real GDP\", \"nominal GDP\", \"industrial production\", \n",
    "            \"manufacturing output\", \"personal consumption\", \"retail sales\", \"job creation\", \"wage growth\", \n",
    "            \"jobless claims\", \"participation rate\", \"productivity\", \"economic indicators\", \"recession\", \"recovery\",\n",
    "            \"economic strength\", \"economic weakness\", \"economic expansion\", \"economic contraction\",\n",
    "            \"macroeconomic conditions\", \"price pressure\", \"cost of living\", \"core inflation\",\n",
    "            \"headline inflation\", \"CPI\", \"PCE\", \"employment cost index\", \"labor force\",],\n",
    "        'context_words': [\n",
    "            \"increase\", \"decrease\", \"improve\", \"deteriorate\", \"strengthen\", \"weaken\",\n",
    "            \"expand\", \"contract\", \"moderate\", \"elevated\", \"stable\", \"volatile\", \"cool\", \n",
    "            \"heat up\", \"surge\", \"plunge\", \"accelerate\", \"slow\", \"trend\", \"fluctuate\",\n",
    "            \"persist\", \"wane\", \"remain strong\", \"remain weak\", \"pick up\", \"soften\",\n",
    "            \"decline\", \"boost\", \"drag\", \"recover\", \"rebound\", \"stagnate\"],'window': 25}\n",
    "    \n",
    "    forward_guidance_keywords = {\n",
    "        'primary_keywords': [\n",
    "            \"future\", \"coming months\", \"coming meetings\", \"outlook\", \"path\", \"trajectory\",\n",
    "            \"forward guidance\", \"anticipate\", \"expect\", \"foresee\", \"project\", \"projection\", \n",
    "            \"forecast\", \"estimate\", \"plan\", \"intention\", \"guidance\", \"signal\",\n",
    "            \"likely\", \"expected path\", \"policy path\", \"ahead\", \"in the near term\",\n",
    "            \"in the medium term\", \"in the long term\", \"forward-looking\", \"views\", \n",
    "            \"baseline scenario\", \"confidence\", \"uncertainty\", \"assumption\", \"time horizon\"],\n",
    "        'context_words': [\n",
    "            \"policy\", \"rate\", \"stance\", \"adjust\", \"accommodation\", \"tightening\",\n",
    "            \"restrictive\", \"neutral\", \"appropriate\", \"Decision\", \"change\", \"maintain\", \"increase\", \n",
    "            \"decrease\", \"reassess\", \"monitor\", \"evaluate\", \"data dependent\", \"determine\", \"meeting\", \n",
    "            \"statement\", \"communication\", \"signal\", \"consider\", \"deliberate\", \"direction\", \"approach\", \n",
    "            \"commitment\", \"resolve\", \"probable\", \"projection error\", \"scenarios\", \"clarity\"],'window': 30}\n",
    "    \n",
    "    balance_sheet_keywords = {\n",
    "        'primary_keywords': [\n",
    "            \"balance sheet\", \"asset purchase\", \"securities\", \"Treasury securities\", \n",
    "            \"agency debt\", \"mortgage-backed securities\", \"MBS\", \"portfolio\", \"holdings\",\n",
    "            \"reinvestment\", \"quantitative easing\", \"QE\", \"runoff\", \"securities holdings\", \"bond purchases\", \n",
    "            \"liquidity operations\", \"Fed assets\", \"balance sheet expansion\", \"balance sheet reduction\", \"QT\", \n",
    "            \"quantitative tightening\", \"securities portfolio\", \"rolloff\", \"maturity schedule\", \"portfolio runoff\",\n",
    "            \"policy normalization\", \"Fed balance sheet\", \"reserve balance\", \"longer-term securities\", \n",
    "            \"monetary base\", \"monetary aggregates\", \"balance sheet unwind\", \"balance sheet management\", \"reserve drainage\"],\n",
    "        'context_words': [\n",
    "            \"increase\", \"decrease\", \"maintain\", \"continue\", \"reduce\", \"expand\",\n",
    "            \"cap\", \"taper\", \"normalize\", \"roll over\", \"maturity\", \"adjust\", \"hold steady\", \n",
    "            \"wind down\", \"phase out\", \"scale back\", \"limit\", \"reinvest\", \"cease\", \"initiate\", \n",
    "            \"accelerate\", \"slow\", \"resume\", \"halt\", \"redeem\", \"run off\", \"implement\", \"conduct\", \n",
    "            \"shift\", \"monitor\", \"evaluate\", \"transition\", \"framework\", \"absorb\", \"inject\", \"drain\"],'window': 25}\n",
    "    \n",
    "    file_count = 0\n",
    "    for file_path in glob.glob(os.path.join(directory, \"*.html\")):\n",
    "        try:\n",
    "            file_count += 1\n",
    "            if file_count % 10 == 0:\n",
    "                print(f\"Processed {file_count} files...\")\n",
    "                \n",
    "            file_name = os.path.basename(file_path)\n",
    "            date_match = re.search(r'(\\d{8})', file_name)\n",
    "            if not date_match:\n",
    "                continue\n",
    "                \n",
    "            date_str = date_match.group(1)\n",
    "            date = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "            \n",
    "            full_text = extract_text_from_html(file_path)\n",
    "            \n",
    "            if is_statement:\n",
    "                text = find_fed_statement_section(full_text)\n",
    "            else:\n",
    "                text = full_text\n",
    "            \n",
    "            cleaned_text = clean_text(text)\n",
    "            \n",
    "            sentences = simple_sentence_tokenize(cleaned_text)\n",
    "\n",
    "            # skipping\n",
    "            if not sentences:\n",
    "                print(f\"No sentences found in {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            monetary_policy_sentences = [s for s in sentences if identify_topic(s, monetary_policy_keywords)]\n",
    "            economic_conditions_sentences = [s for s in sentences if identify_topic(s, economic_conditions_keywords)]\n",
    "            forward_guidance_sentences = [s for s in sentences if identify_topic(s, forward_guidance_keywords)]\n",
    "            balance_sheet_sentences = [s for s in sentences if identify_topic(s, balance_sheet_keywords)]\n",
    "            \n",
    "            result = {\n",
    "                'date': date,\n",
    "                'document_type': 'statement' if is_statement else 'intermeeting',\n",
    "                'file_path': file_path,\n",
    "                'full_text': cleaned_text,\n",
    "                'monetary_policy_text': ' '.join(monetary_policy_sentences),\n",
    "                'economic_conditions_text': ' '.join(economic_conditions_sentences),\n",
    "                'forward_guidance_text': ' '.join(forward_guidance_sentences),\n",
    "                'balance_sheet_text': ' '.join(balance_sheet_sentences),\n",
    "                'num_sentences_total': len(sentences),\n",
    "                'num_monetary_policy': len(monetary_policy_sentences),\n",
    "                'num_economic_conditions': len(economic_conditions_sentences),\n",
    "                'num_forward_guidance': len(forward_guidance_sentences),\n",
    "                'num_balance_sheet': len(balance_sheet_sentences)\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11883946-b84b-4c52-b3ad-57ac140bcda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FED statements!\n",
      "Processed 10 files...\n",
      "Processed 20 files...\n",
      "Processed 30 files...\n",
      "Processed 40 files...\n",
      "Processed 50 files...\n",
      "Processed 60 files...\n",
      "Processed 70 files...\n",
      "Processed 80 files...\n",
      "Processed 90 files...\n",
      "Processed 100 files...\n",
      "Processed 110 files...\n",
      "Processed 120 files...\n",
      "Processed 130 files...\n",
      "Processed 140 files...\n",
      "Processed 150 files...\n",
      "Processed 160 files...\n",
      "Processed 170 files...\n",
      "Processed 180 files...\n",
      "Processed 190 files...\n",
      "Processed 200 files...\n",
      "Processing intermeeting minutes!\n",
      "Processed 10 files...\n",
      "Processed 20 files...\n",
      "Processed 30 files...\n",
      "Processed 40 files...\n",
      "Processed 50 files...\n",
      "Processed 60 files...\n",
      "Processed 70 files...\n",
      "Processed 80 files...\n",
      "Processed 90 files...\n",
      "Processed 100 files...\n",
      "Processed 110 files...\n",
      "Processed 120 files...\n",
      "Processed 130 files...\n",
      "Processed 140 files...\n",
      "Processed 150 files...\n",
      "Processed 160 files...\n",
      "Processed 170 files...\n",
      "Processed 180 files...\n",
      "Processed 190 files...\n",
      "Processed 200 files...\n",
      "Combining datasets!\n",
      "Text processing complete!\n",
      "Processed 200 statements and 200 intermeeting minutes.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to process all files.\"\"\"\n",
    "    print(\"Processing FED statements!\")\n",
    "    statements_df = process_files(ANNOUNCEMENTS_DIR, is_statement=True)\n",
    "    statements_df.to_csv(os.path.join(PROCESSED_DIR, \"processed_statements.csv\"), index=False)\n",
    "    \n",
    "    print(\"Processing intermeeting minutes!\")\n",
    "    intermeeting_df = process_files(INTERMEETING_DIR, is_statement=False)\n",
    "    intermeeting_df.to_csv(os.path.join(PROCESSED_DIR, \"processed_intermeeting.csv\"), index=False)\n",
    "    \n",
    "    print(\"Combining datasets!\")\n",
    "    combined_df = pd.concat([statements_df, intermeeting_df], ignore_index=True)\n",
    "    combined_df.sort_values('date', inplace=True)\n",
    "    combined_df.to_csv(os.path.join(PROCESSED_DIR, \"processed_fed_documents.csv\"), index=False)\n",
    "    \n",
    "    print(\"Text processing complete!\")\n",
    "    print(f\"Processed {len(statements_df)} statements and {len(intermeeting_df)} intermeeting minutes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86a667-805c-4318-a848-575dbd4c04a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
